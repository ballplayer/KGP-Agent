import os
import json
import time
import numpy as np
import networkx as nx
import google.generativeai as genai

# ================= é…ç½®åŒºåŸŸ =================
os.environ["HTTP_PROXY"] = "http://127.0.0.1:7890"
os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7890"
# è¯·æ›¿æ¢ä¸ºä½ çš„å®é™… API Key
# ä½¿ç”¨ç¯å¢ƒå˜é‡è¯»å– API Keyï¼Œé¿å…åœ¨ä»£ç ä¸­ç¡¬ç¼–ç å¯†é’¥
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    print("âš ï¸ ç¯å¢ƒå˜é‡ GOOGLE_API_KEY æœªè®¾ç½®ã€‚LLM/Embedding è°ƒç”¨å°†ä¼šå¤±è´¥ã€‚\n   è¯·åœ¨è¿è¡Œå‰é€šè¿‡ PowerShell: $env:GOOGLE_API_KEY=\"<your_key>\" è®¾ç½®ã€‚")
else:
    genai.configure(api_key=GOOGLE_API_KEY)


class HybridPruner:
    def __init__(self):
        self.model_llm = genai.GenerativeModel('gemini-2.5-flash')
        # ä½¿ç”¨ embedding æ¨¡å‹è¿›è¡Œå‘é‡åŒ–
        self.model_embed = "models/text-embedding-004"

    def _calculate_cosine_similarity(self, vec_a, vec_b):
        """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        norm_a = np.linalg.norm(vec_a)
        norm_b = np.linalg.norm(vec_b)
        if norm_a == 0 or norm_b == 0:
            return 0.0
        return np.dot(vec_a, vec_b) / (norm_a * norm_b)

    def step_1_topology_pruning(self, data):
        """
        ç­–ç•¥1: æ‹“æ‰‘ç»“æ„å‰ªæ
        - åˆ©ç”¨ NetworkX æ„å»ºå›¾ã€‚
        - åˆ é™¤å­¤ç«‹èŠ‚ç‚¹ (Degree=0)ã€‚
        - åˆ é™¤éæ ¸å¿ƒè¿é€šåˆ†é‡ (å¦‚æœå›¾ä¸ä»…æ˜¯ä¸€ä¸ªæ•´ä½“)ã€‚
        - è§„åˆ™è¿‡æ»¤: åˆ é™¤åœ¨ Schema é»‘åå•ä¸­çš„ç±»å‹ (å¦‚ 'èƒŒæ™¯', 'å™ªéŸ³')ã€‚
        """
        print("   âœ‚ï¸ [Step 1] æ‰§è¡Œæ‹“æ‰‘ç»“æ„å‰ªæ...")

        entities = {e['id']: e for e in data['entities']}
        relations = data['relationships']

        # 1.1 æ„å»ºå›¾
        G = nx.Graph()  # ä½¿ç”¨æ— å‘å›¾è®¡ç®—è¿é€šæ€§
        for e_id in entities.keys():
            G.add_node(e_id)
        for r in relations:
            G.add_edge(r['source'], r['target'])

        initial_count = len(G.nodes)

        # 1.2 ç§»é™¤å­¤ç«‹èŠ‚ç‚¹ (Degree = 0)
        isolates = list(nx.isolates(G))
        G.remove_nodes_from(isolates)

        # 1.3 ç±»å‹é»‘åå•è¿‡æ»¤ (æ¨¡æ‹Ÿ Schema çº¦æŸ)
        # å‡è®¾æˆ‘ä»¬éœ€è¦ç§»é™¤è¿™äº›ç±»å‹çš„å®ä½“ï¼Œé™¤éå®ƒä»¬æ˜¯è¿æ¥åº¦å¾ˆé«˜çš„æ¢çº½
        blacklist_types = ["èƒŒæ™¯", "å™ªéŸ³", "ç¯å¢ƒæè¿°", "ä¿®é¥°è¯"]
        nodes_to_remove = []
        for node in G.nodes:
            if node in entities:
                e_type = entities[node]['type']
                degree = G.degree[node]
                # å¦‚æœæ˜¯é»‘åå•ç±»å‹ï¼Œä¸”ä¸æ˜¯æ ¸å¿ƒæ¢çº½(åº¦æ•°<=1)ï¼Œåˆ™åˆ é™¤
                if any(bt in e_type for bt in blacklist_types) and degree <= 1:
                    nodes_to_remove.append(node)

        G.remove_nodes_from(nodes_to_remove)

        # 1.4 é‡å»ºæ•°æ®
        valid_nodes = set(G.nodes)
        new_entities = [e for e in data['entities'] if e['id'] in valid_nodes]
        new_relations = [r for r in data['relationships']
                         if r['source'] in valid_nodes and r['target'] in valid_nodes]

        print(f"      - ç§»é™¤äº† {initial_count - len(valid_nodes)} ä¸ªæ‹“æ‰‘å†—ä½™/å­¤ç«‹å®ä½“ã€‚")
        return {"entities": new_entities, "relationships": new_relations}

    def step_2_semantic_fusion(self, data):
        """
        ç­–ç•¥2: åŸºäºå‘é‡åµŒå…¥çš„å±æ€§èåˆ
        - å¯¹åŒä¸€ä¸ªå®ä½“çš„æ‰€æœ‰å±æ€§è¿›è¡Œ Embeddingã€‚
        - è®¡ç®—ä¸¤ä¸¤ç›¸ä¼¼åº¦ï¼Œå¦‚æœ > 0.9 åˆ™åˆå¹¶ã€‚
        """
        print("   ğŸ§¬ [Step 2] æ‰§è¡Œå‘é‡è¯­ä¹‰èåˆ (å±æ€§å»é‡)...")

        new_entities = []

        for entity in data['entities']:
            attrs = entity.get('attributes', {})
            if len(attrs) < 2:
                new_entities.append(entity)
                continue

            keys = list(attrs.keys())
            # æ„é€ è¯­ä¹‰æ–‡æœ¬: "å±æ€§å: å±æ€§å€¼"
            texts = [f"{k}: {attrs[k]}" for k in keys]

            try:
                # æ‰¹é‡è·å–å‘é‡ (Gemini Embedding API)
                embeddings = genai.embed_content(
                    model=self.model_embed,
                    content=texts,
                    task_type="semantic_similarity"
                )['embedding']

                # æ ‡è®°éœ€è¦åˆ é™¤çš„å†—ä½™å±æ€§ Key
                keys_to_remove = set()

                for i in range(len(keys)):
                    if keys[i] in keys_to_remove: continue
                    for j in range(i + 1, len(keys)):
                        if keys[j] in keys_to_remove: continue

                        # è®¡ç®—ç›¸ä¼¼åº¦
                        sim = self._calculate_cosine_similarity(embeddings[i], embeddings[j])

                        # é˜ˆå€¼åˆ¤å®š (0.9 è¡¨ç¤ºéå¸¸ç›¸ä¼¼)
                        if sim > 0.9:
                            # ä¿ç•™é€»è¾‘: ä¿ç•™æè¿°æ›´çŸ­çš„(æ›´ç²¾ç®€)ï¼Œæˆ–è€…ä¿ç•™ç‰¹å®šå…³é”®è¯
                            # è¿™é‡Œç®€å•æ¼”ç¤ºï¼šä¿ç•™ iï¼Œåˆ é™¤ j
                            keys_to_remove.add(keys[j])
                            # print(f"      - [èåˆ] '{texts[j]}' (å†—ä½™) -> è¢« '{texts[i]}' è¦†ç›– (ç›¸ä¼¼åº¦: {sim:.2f})")

                # é‡å»ºå±æ€§å­—å…¸
                new_attrs = {k: v for k, v in attrs.items() if k not in keys_to_remove}
                entity['attributes'] = new_attrs

            except Exception as e:
                print(f"      âš ï¸ å‘é‡è®¡ç®—å‡ºé”™ (å¯èƒ½æ˜¯é…é¢é™åˆ¶): {e}")
                pass

            new_entities.append(entity)

        data['entities'] = new_entities
        return data

    def step_3_llm_refinement(self, data):
        """
        ç­–ç•¥3: LLM æœ€ç»ˆç²¾ä¿®
        - å°†æ¸…æ´—è¿‡çš„æ•°æ®å‘ç»™ LLMï¼Œåšæœ€åçš„é€»è¾‘åˆ¤æ–­ã€‚
        - æ­¤æ—¶ Token æ¶ˆè€—å·²å¤§å¹…é™ä½ã€‚
        """
        print("   ğŸ§  [Step 3] æ‰§è¡Œ LLM æœ€ç»ˆé€»è¾‘ç²¾ä¿®...")

        json_str = json.dumps(data, ensure_ascii=False, indent=2)

        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±ä¸“å®¶ã€‚ä¸Šè¿°æ•°æ®å·²ç»ç»è¿‡äº†åˆæ­¥çš„æ‹“æ‰‘æ¸…æ´—å’Œå‘é‡å»é‡ã€‚
        ç°åœ¨çš„ä»»åŠ¡æ˜¯è¿›è¡Œæœ€åçš„**é€»è¾‘å‰ªæ**ã€‚

        è¾“å…¥æ•°æ®:
        {json_str}

        è¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
        1. **æ£€æŸ¥å…³ç³»åˆç†æ€§**: åˆ é™¤é€»è¾‘ä¸Šä¸é€šé¡ºæˆ–é”™è¯¯çš„å…³ç³»ã€‚
        2. **å®ä½“åˆå¹¶**: å¦‚æœå›¾è°±ä¸­å­˜åœ¨ä¸¤ä¸ªä¸åŒ ID ä½†æŒ‡ä»£åŒä¸€äº‹ç‰©çš„å®ä½“ï¼ˆä¾‹å¦‚ "CEO" å’Œ "é©¬æ–¯å…‹" å¦‚æœåˆ†å¼€èŠ‚ç‚¹äº†ï¼‰ï¼Œè¯·å°†å®ƒä»¬åˆå¹¶ã€‚
        3. **æ ¼å¼è§„èŒƒåŒ–**: ç¡®ä¿è¾“å‡ºæ ‡å‡†çš„ JSONã€‚

        è¯·ç›´æ¥è¿”å›ä¼˜åŒ–åçš„ JSON æ•°æ®ï¼Œä¸è¦åŒ…å« Markdown æ ‡è®°ã€‚
        """

        try:
            response = self.model_llm.generate_content(
                prompt,
                generation_config={"response_mime_type": "application/json"}
            )
            return json.loads(response.text)
        except Exception as e:
            print(f"      âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
            return data  # å¦‚æœå¤±è´¥ï¼Œè¿”å›ä¸Šä¸€æ­¥çš„ç»“æœ

    def run(self, input_data):
        """æµæ°´çº¿å…¥å£"""
        # Step 1: æ‹“æ‰‘æ¸…æ´—
        data_s1 = self.step_1_topology_pruning(input_data)

        # Step 2: å‘é‡èåˆ
        data_s2 = self.step_2_semantic_fusion(data_s1)

        # Step 3: LLM æœ€ç»ˆç¡®è®¤
        data_final = self.step_3_llm_refinement(data_s2)

        return data_final


# ================= å…¼å®¹æ—§ç‰ˆæ¥å£ =================
# ä¸ºäº†è®© main_pipeline.py èƒ½ç›´æ¥è°ƒç”¨ï¼Œä¿æŒå‡½æ•°åä¸€è‡´
def prune_graph_with_gemini(input_data):
    pruner = HybridPruner()
    return pruner.run(input_data)


# ================= å¯è§†åŒ–å‡½æ•° (ä¿æŒä¸å˜) =================
def visualize_kg(data, title="Optimized Pruned Graph"):
    if not data: return
    G = nx.DiGraph()
    for entity in data.get("entities", []):
        node_label = f"{entity['id']}\n({entity['type']})"
        G.add_node(entity['id'], label=node_label, type=entity['type'])
    for rel in data.get("relationships", []):
        G.add_edge(rel["source"], rel["target"], label=rel["relation"])

    plt.figure(figsize=(10, 8))
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'Microsoft YaHei', 'sans-serif']
    plt.rcParams['axes.unicode_minus'] = False

    pos = nx.spring_layout(G, k=0.6, iterations=50, seed=42)
    nx.draw_networkx_nodes(G, pos, node_size=3500, node_color='lightgreen', alpha=0.9, edgecolors='gray')
    nx.draw_networkx_labels(G, pos, font_size=9, font_weight="bold", font_family='sans-serif')
    nx.draw_networkx_edges(G, pos, width=2, alpha=0.6, edge_color='gray', arrowstyle='-|>', arrowsize=20)
    edge_labels = {(u, v): d['label'] for u, v, d in G.edges(data=True)}
    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=9, label_pos=0.5)

    plt.title(title, fontsize=15)
    plt.axis('off')
    plt.savefig("KG_P_visualization.png", format="png", dpi=300)
    plt.show()


# ================= å•å…ƒæµ‹è¯• =================
if __name__ == "__main__":
    import matplotlib.pyplot as plt

    # æ¨¡æ‹Ÿæ•°æ®ï¼šåŒ…å«å†—ä½™å±æ€§å’Œå­¤ç«‹ç‚¹
    dummy_data = {
        "entities": [
            {
                "id": "æœºå™¨äºº",
                "type": "äº§å“",
                "attributes": {
                    "çŠ¶æ€": "å¼€å‘ä¸­",
                    "å¼€å‘è¿›åº¦": "æ­£åœ¨ç ”å‘",  # è¯­ä¹‰é‡å¤
                    "é¢œè‰²": "ç™½è‰²"
                }
            },
            {"id": "å™ªéŸ³ç‚¹A", "type": "èƒŒæ™¯", "attributes": {}},  # å­¤ç«‹ç‚¹
            {"id": "å±•ä¼š", "type": "åœºåˆ", "attributes": {}}
        ],
        "relationships": [
            {"source": "æœºå™¨äºº", "target": "å±•ä¼š", "relation": "å±•ç¤ºäº"}
        ]
    }

    print("--- å¼€å§‹æµ‹è¯•ä¼˜åŒ–ç‰ˆå‰ªæ ---")
    optimized_kg = prune_graph_with_gemini(dummy_data)

    print("\n--- ç»“æœ ---")
    print(json.dumps(optimized_kg, ensure_ascii=False, indent=2))